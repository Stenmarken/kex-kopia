{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import time\n",
    "import collections\n",
    "import keras\n",
    "import random\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import MaxPooling1D, Conv1D, Conv2D, MaxPooling2D, GlobalAveragePooling2D, LSTM, ELU, Bidirectional, Attention\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, CuDNNLSTM\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras_tuner\n",
    "from keras import backend as K\n",
    "import shutil\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model_type = 'cnn'\n",
    "global_folder_name = 'cnn_base_model'\n",
    "file_suffix = 'weyo1'\n",
    "global_batch_size = 32\n",
    "global_epochs = 50\n",
    "global_learning_rate = 0.0001\n",
    "own_file_path = os.getcwd() \n",
    "global_hyperparameter_folder_name = '5_2_lstm_filter_size_dropout' #lstm_filter_size_dropout1' #'cnn_filter_size_dropout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instrument_code(filename):\n",
    "    \"\"\"\n",
    "    Function that takes in a filename and returns instrument based on naming convention\n",
    "    \"\"\"\n",
    "    # Synth lead borttagen. id = 9\n",
    "    class_names=['bass', 'brass', 'flute', 'guitar', \n",
    "             'keyboard', 'mallet', 'organ', 'reed', \n",
    "             'string', 'vocal']\n",
    "    \n",
    "    for name in class_names:\n",
    "        if name in filename:\n",
    "            return class_names.index(name)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        X_train_full  = pickle.load(f)\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for (key, value) in X_train_full:\n",
    "        X_train.append(value)\n",
    "        y_train.append(instrument_code(key))\n",
    "\n",
    "    X_train_numpy = np.array(X_train)\n",
    "    y_train_numpy = np.array(y_train)\n",
    "    return (X_train_numpy, y_train_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_old_test_data(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        old_X_test_full = pickle.load(f)\n",
    "\n",
    "    old_X_test = []\n",
    "    old_y_test = []\n",
    "\n",
    "    for(key, value) in old_X_test_full.items():\n",
    "        #temporal_value = np.mean(value, axis = 1)\n",
    "        old_X_test.append(value)\n",
    "        old_y_test.append(instrument_code(key))\n",
    "        \n",
    "    old_y_test_numpy = np.asarray(old_y_test)\n",
    "    old_X_test_numpy = np.asarray(old_X_test)\n",
    "    return (old_X_test_numpy, old_y_test_numpy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred):\n",
    "    print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_cnn_model(hp):\n",
    "    \"\"\"\n",
    "    Function that builds a CNN model with optimized hyperparameters including filter size and dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                    activation='relu', padding='same', input_shape=(126, 13, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_1', values = [32, 64, 128]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_1', values = [32, 64, 128]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_2', values = [32, 64, 128, 256]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=global_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_lstm_model(hp):\n",
    "    \"\"\"\n",
    "    Function that builds a LSTM model with optimized hyperparameters including filter size and dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    #model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True), input_shape=(126, 128)))\n",
    "    #model.add(CuDNNLSTM(4, input_shape=(126, 128), return_sequences=True))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, input_shape=(126, 13), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_1', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_1', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_2', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_2', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    #model.add(LSTM(16, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=global_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model():    \n",
    "    input_shape = (126, 13, 1)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128,(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    model = Sequential()\n",
    "    lstm_base_dropout_rate = 0.25\n",
    "    #model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True), input_shape=(126, 128)))\n",
    "    #model.add(CuDNNLSTM(4, input_shape=(126, 128), return_sequences=True))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, input_shape=(126, 13), return_sequences=True))\n",
    "    model.add(Dropout(lstm_base_dropout_rate))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(lstm_base_dropout_rate))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(lstm_base_dropout_rate))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(lstm_base_dropout_rate))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    #model.add(LSTM(16, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(history, acc_file_path, loss_file_path):\n",
    "    # Taget från https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(acc_file_path)\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(loss_file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path):    \n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "    result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "\n",
    "    df_cm = pd.DataFrame(result, index = [i for i in [\"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\", \"mallet\", \"organ\", \"reed\", \"string\", \"vocal\"]],\n",
    "                    columns = [i for i in [\"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\", \"mallet\", \"organ\", \"reed\", \"string\", \"vocal\"]])\n",
    "\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(confusion_matrix_file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn(fold_index):\n",
    "\n",
    "    old_test_file = 'CustomDataFull/testdata2000.pkl'\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    old_X_test, old_y_test = get_old_test_data(old_test_file)\n",
    "\n",
    "    fold_path = 'results/cnn_results/' + global_folder_name + '/' + str(fold_index) + file_suffix + '/'\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "\n",
    "    model = build_cnn_model()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    history = model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=global_batch_size, epochs=global_epochs, shuffle=True, verbose=2)\n",
    "    model.save(fold_path + str(fold_index) + 'model_' + file_suffix + '.h5')\n",
    "\n",
    "    history_name = fold_path + str(fold_index) + 'history_' + file_suffix + '.history'\n",
    "\n",
    "    with open(history_name, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "\n",
    "    print(\"\\n\\ny_test : \", y_test)\n",
    "    print(\"\\n\\ny_prediction : \", y_prediction)\n",
    "\n",
    "    f1_scores = metrics.classification_report(y_test, y_prediction, digits=3)\n",
    "\n",
    "    original_scores = model.evaluate(old_X_test, old_y_test, verbose=1)\n",
    "    original_y_prediction = model.predict(old_X_test)\n",
    "    original_y_prediction = np.argmax(original_y_prediction, axis = 1)\n",
    "    original_f1_scores = metrics.classification_report(old_y_test, original_y_prediction, digits=3)\n",
    "\n",
    "    score_file = fold_path + str(fold_index) + 'score_' + file_suffix + '.txt'\n",
    "    with open(score_file, 'w') as f:\n",
    "        f.write('fold: ' + str(fold_index) + '\\n')\n",
    "        f.write('scores: ' + str(scores) + '\\n')\n",
    "        f.write('scores with original testdata: ' + str(original_scores) + '\\n')\n",
    "        f.write('f1_scores:\\n' + str(f1_scores) + '\\n')\n",
    "        f.write('f1_scores with original testdata:\\n' + str(original_f1_scores) + '\\n')\n",
    "\n",
    "    loss_file_path = fold_path + str(fold_index) + 'loss_' + file_suffix + '.png'\n",
    "    acc_file_path = fold_path + str(fold_index) + 'acc_' + file_suffix + '.png'\n",
    "    plot_acc_loss(history.history, acc_file_path, loss_file_path)\n",
    "\n",
    "    confusion_matrix_file_path = fold_path + str(fold_index) + 'confusion_matrix_' + file_suffix + '.png'\n",
    "    plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm(fold_index):\n",
    "    old_test_file = 'CustomDataFull/testdata2000.pkl'\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    old_X_test, old_y_test = get_old_test_data(old_test_file)\n",
    "\n",
    "    fold_path = 'results/lstm_results/' + global_folder_name + '/' + str(fold_index) + file_suffix + '/'\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "\n",
    "    model = build_lstm_model()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy'])\n",
    "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy', \n",
    "    #              tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.F1Score(num_classes=10, average='macro',\n",
    "    #              threshold=0.5)])\n",
    "    \n",
    "    model.summary()\n",
    "    history = model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=global_batch_size, epochs=global_epochs, shuffle=True, verbose=2)\n",
    "    model.save(fold_path + str(fold_index) + 'model_' + file_suffix + '.h5')\n",
    "\n",
    "    history_name = fold_path + str(fold_index) + 'history_' + file_suffix + '.history'\n",
    "\n",
    "    with open(history_name, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "    f1_scores = metrics.classification_report(y_test, y_prediction, digits=3)\n",
    "\n",
    "    original_scores = model.evaluate(old_X_test, old_y_test, verbose=1)\n",
    "    original_y_prediction = model.predict(old_X_test)\n",
    "    original_y_prediction = np.argmax(original_y_prediction, axis = 1)\n",
    "    original_f1_scores = metrics.classification_report(old_y_test, original_y_prediction, digits=3)\n",
    "    \n",
    "    score_file = fold_path + str(fold_index) + 'score_' + file_suffix + '.txt'\n",
    "    with open(score_file, 'w') as f:\n",
    "        f.write('fold: ' + str(fold_index) + '\\n')\n",
    "        f.write('scores: ' + str(scores) + '\\n')\n",
    "        f.write('scores with original testdata: ' + str(original_scores) + '\\n')\n",
    "        f.write('f1_scores: ' + str(f1_scores) + '\\n')\n",
    "        f.write('f1_scores with original testdata: ' + str(original_f1_scores) + '\\n')\n",
    "\n",
    "    loss_file_path = fold_path + str(fold_index) + 'loss_' + file_suffix + '.png'\n",
    "    acc_file_path = fold_path + str(fold_index) + 'acc_' + file_suffix + '.png'\n",
    "    plot_acc_loss(history.history, acc_file_path, loss_file_path)\n",
    "\n",
    "    confusion_matrix_file_path = fold_path + str(fold_index) + 'confusion_matrix_' + file_suffix + '.png'\n",
    "    plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_cnn():\n",
    "    \"\"\"\n",
    "    OBS! Efter en körning så printas summaryn över resultaten gällande de olika hyperparametrarna. Spara dom!\n",
    "    \"\"\"\n",
    "    tuner = keras_tuner.RandomSearch(hyperparameter_cnn_model, overwrite=True, objective='val_accuracy', max_trials=5, executions_per_trial=2, directory='hyperparameters', project_name=global_hyperparameter_folder_name)\n",
    "    tuner.search_space_summary()\n",
    "    fold_index = 0 # Godtyckligt val!!!\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    tuner.search(X_train, y_train, epochs=global_epochs, batch_size=global_batch_size, validation_data=(X_valid, y_valid), shuffle=True, verbose=2, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='hyperparameters/' + global_hyperparameter_folder_name)])\n",
    "    \n",
    "    print(tuner.get_best_hyperparameters()[0].values)\n",
    "\n",
    "    hyperparameter_log_name = 'hyperparameters/' + global_hyperparameter_folder_name + '/log.txt'\n",
    "\n",
    "    with open(hyperparameter_log_name, 'w') as f:\n",
    "        f.write('best hyperparameters: ' + str(tuner.get_best_hyperparameters()[0].values) + '\\n')\n",
    "        f.write(str(tuner.results_summary()) + '\\n')\n",
    "\n",
    "    tuner.results_summary()\n",
    "\n",
    "    copy_file = 'hyperparameters/' + global_hyperparameter_folder_name + '/' + 'cnn-hyperparameter_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lstm():\n",
    "    \"\"\"\n",
    "    OBS! Efter en körning så printas summaryn över resultaten gällande de olika hyperparametrarna. Spara dom!\n",
    "    \"\"\"\n",
    "    tuner = keras_tuner.RandomSearch(hyperparameter_lstm_model, overwrite=True, objective='val_accuracy', max_trials=5, executions_per_trial=2, directory='hyperparameters', project_name=global_hyperparameter_folder_name)\n",
    "    tuner.search_space_summary()\n",
    "    fold_index = 0 # Godtyckligt val!!!\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    tuner.search(X_train, y_train, epochs=global_epochs, batch_size=global_batch_size, validation_data=(X_valid, y_valid), shuffle=True, verbose=2, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='hyperparameters/' + global_hyperparameter_folder_name)])\n",
    "    \n",
    "    print(tuner.get_best_hyperparameters()[0].values)\n",
    "\n",
    "    hyperparameter_log_name = 'hyperparameters/' + global_hyperparameter_folder_name + '/log.txt'\n",
    "\n",
    "    with open(hyperparameter_log_name, 'w') as f:\n",
    "        f.write('best hyperparameters: ' + str(tuner.get_best_hyperparameters()[0].values) + '\\n')\n",
    "    \n",
    "    tuner.results_summary()\n",
    "    \n",
    "    copy_file = 'hyperparameters/' + global_hyperparameter_folder_name + '/' + 'lstm-hyperparameter_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_5_fold_cross_validation():\n",
    "    path = 'results/cnn_results/' + global_folder_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    copy_file = 'results/cnn_results/' + global_folder_name + '/' + 'copied_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        run_cnn(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_5_fold_cross_validation():\n",
    "    path = 'results/lstm_results/' + global_folder_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    copy_file = 'results/lstm_results/' + global_folder_name + '/' + 'copied_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        run_lstm(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [01h 28m 51s]\n",
      "val_accuracy: 0.9606857001781464\n",
      "\n",
      "Best val_accuracy So Far: 0.9656571447849274\n",
      "Total elapsed time: 06h 56m 35s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "{'dropout_lstm': 0.30000000000000004, 'lstm_layer_units_1': 32, 'lstm_layer_units_2': 128}\n",
      "Results summary\n",
      "Results in hyperparameters/5_2_lstm_filter_size_dropout\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 3 summary\n",
      "Hyperparameters:\n",
      "dropout_lstm: 0.30000000000000004\n",
      "lstm_layer_units_1: 32\n",
      "lstm_layer_units_2: 128\n",
      "Score: 0.9656571447849274\n",
      "\n",
      "Trial 4 summary\n",
      "Hyperparameters:\n",
      "dropout_lstm: 0.4\n",
      "lstm_layer_units_1: 64\n",
      "lstm_layer_units_2: 128\n",
      "Score: 0.9606857001781464\n",
      "\n",
      "Trial 0 summary\n",
      "Hyperparameters:\n",
      "dropout_lstm: 0.4\n",
      "lstm_layer_units_1: 128\n",
      "lstm_layer_units_2: 64\n",
      "Score: 0.9588000178337097\n",
      "\n",
      "Trial 1 summary\n",
      "Hyperparameters:\n",
      "dropout_lstm: 0.2\n",
      "lstm_layer_units_1: 32\n",
      "lstm_layer_units_2: 32\n",
      "Score: 0.9583999812602997\n",
      "\n",
      "Trial 2 summary\n",
      "Hyperparameters:\n",
      "dropout_lstm: 0.30000000000000004\n",
      "lstm_layer_units_1: 64\n",
      "lstm_layer_units_2: 32\n",
      "Score: 0.9566857218742371\n"
     ]
    }
   ],
   "source": [
    "if (global_model_type == 'cnn'):\n",
    "    print('Start running cnn')\n",
    "    run_cnn_5_fold_cross_validation()\n",
    "elif (global_model_type == 'lstm'):\n",
    "    print('Start running lstm')\n",
    "    run_lstm_5_fold_cross_validation()\n",
    "elif (global_model_type == 'tune_cnn'):\n",
    "    print('Start tuning cnn')\n",
    "    tune_cnn()\n",
    "elif (global_model_type == 'tune_lstm'):\n",
    "    print('Start tuning lstm')\n",
    "    tune_lstm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
