{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import time\n",
    "import collections\n",
    "import keras\n",
    "import random\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import MaxPooling1D, Conv1D, Conv2D, MaxPooling2D, GlobalAveragePooling2D, LSTM, ELU, Bidirectional, Attention\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, CuDNNLSTM\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras_tuner\n",
    "from keras import backend as K\n",
    "import shutil\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model_type = 'cnn'\n",
    "global_folder_name = 'cnn_base_model_new_dr_lr'\n",
    "file_suffix = 'weyo1'\n",
    "global_batch_size = 32\n",
    "global_epochs = 50\n",
    "global_learning_rate = 0.001\n",
    "own_file_path = os.getcwd() \n",
    "global_hyperparameter_folder_name = 'lstm_new_lr_12_05'  #lstm_filter_size_dropout1' #'cnn_filter_size_dropout'\n",
    "optimized_model = False # Om satt till True körs den hyperparameter tune:ade modellen. Annars base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instrument_code(filename):\n",
    "    \"\"\"\n",
    "    Function that takes in a filename and returns instrument based on naming convention\n",
    "    \"\"\"\n",
    "    # Synth lead borttagen. id = 9\n",
    "    class_names=['bass', 'brass', 'flute', 'guitar', \n",
    "             'keyboard', 'mallet', 'organ', 'reed', \n",
    "             'string', 'vocal']\n",
    "    \n",
    "    for name in class_names:\n",
    "        if name in filename:\n",
    "            return class_names.index(name)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        X_train_full  = pickle.load(f)\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for (key, value) in X_train_full:\n",
    "        X_train.append(value)\n",
    "        y_train.append(instrument_code(key))\n",
    "\n",
    "    X_train_numpy = np.array(X_train)\n",
    "    y_train_numpy = np.array(y_train)\n",
    "    return (X_train_numpy, y_train_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_old_test_data(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        old_X_test_full = pickle.load(f)\n",
    "\n",
    "    old_X_test = []\n",
    "    old_y_test = []\n",
    "\n",
    "    for(key, value) in old_X_test_full.items():\n",
    "        #temporal_value = np.mean(value, axis = 1)\n",
    "        old_X_test.append(value)\n",
    "        old_y_test.append(instrument_code(key))\n",
    "        \n",
    "    old_y_test_numpy = np.asarray(old_y_test)\n",
    "    old_X_test_numpy = np.asarray(old_X_test)\n",
    "    return (old_X_test_numpy, old_y_test_numpy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred):\n",
    "    print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_cnn_model(hp):\n",
    "    \"\"\"\n",
    "    Function that builds a CNN model with optimized hyperparameters including filter size and dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                    activation='relu', padding='same', input_shape=(126, 13, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_1', values = [32, 64, 128]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_1', values = [32, 64, 128]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_2', values = [32, 64, 128, 256]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=global_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_lstm_model(hp):\n",
    "    \"\"\"\n",
    "    Function that builds a LSTM model with optimized hyperparameters including filter size and dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    #model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True), input_shape=(126, 128)))\n",
    "    #model.add(CuDNNLSTM(4, input_shape=(126, 128), return_sequences=True))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, input_shape=(126, 13), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_1', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_1', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_2', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_2', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    #model.add(LSTM(16, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=global_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model():    \n",
    "    input_shape = (126, 13, 1)\n",
    "    cnn_dropout_rate = 0.5\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(cnn_dropout_rate))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(cnn_dropout_rate))\n",
    "\n",
    "    model.add(Conv2D(128,(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimized_cnn_model():\n",
    "    input_shape = (126, 13, 1)\n",
    "    dropout_1 = 0.2 \n",
    "    conv_2_filters_1 = 128\n",
    "    conv_2_filters_2 = 128\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_1))\n",
    "\n",
    "    model.add(Conv2D(conv_2_filters_1, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(conv_2_filters_1, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_1))\n",
    "\n",
    "    model.add(Conv2D(conv_2_filters_2,(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Dropout(dropout_1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    model = Sequential()\n",
    "    #model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True), input_shape=(126, 128)))\n",
    "    #model.add(CuDNNLSTM(4, input_shape=(126, 128), return_sequences=True))\n",
    "    #model.add(Dropout(0.5))\n",
    "    lstm_dropout_rate = 0.25\n",
    "    model.add(LSTM(32, input_shape=(126, 13), return_sequences=True))\n",
    "    model.add(Dropout(lstm_dropout_rate))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(lstm_dropout_rate))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(lstm_dropout_rate))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(lstm_dropout_rate))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    #model.add(LSTM(16, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimized_lstm_model():\n",
    "    dropout_lstm = 0.3\n",
    "    lstm_layer_units_1 = 32\n",
    "    lstm_layer_units_2 = 128\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=(126, 13), return_sequences=True))\n",
    "    model.add(Dropout(dropout_lstm))\n",
    "    model.add(LSTM(lstm_layer_units_1, return_sequences=True))\n",
    "    model.add(Dropout(dropout_lstm))\n",
    "    model.add(LSTM(lstm_layer_units_1, return_sequences=True))\n",
    "    model.add(Dropout(dropout_lstm))\n",
    "    model.add(LSTM(lstm_layer_units_2, return_sequences=True))\n",
    "    model.add(Dropout(dropout_lstm))\n",
    "    model.add(LSTM(lstm_layer_units_2, return_sequences=True))\n",
    "    #model.add(LSTM(16, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(history, acc_file_path, loss_file_path):\n",
    "    # Taget från https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(acc_file_path)\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(loss_file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path):    \n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "    result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "\n",
    "    df_cm = pd.DataFrame(result, index = [i for i in [\"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\", \"mallet\", \"organ\", \"reed\", \"string\", \"vocal\"]],\n",
    "                    columns = [i for i in [\"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\", \"mallet\", \"organ\", \"reed\", \"string\", \"vocal\"]])\n",
    "\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(confusion_matrix_file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn(fold_index):\n",
    "\n",
    "    old_test_file = 'CustomDataFull/testdata2000.pkl'\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    old_X_test, old_y_test = get_old_test_data(old_test_file)\n",
    "\n",
    "    fold_path = 'results/cnn_results/' + global_folder_name + '/' + str(fold_index) + file_suffix + '/'\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "\n",
    "    if optimized_model:\n",
    "        model = build_optimized_cnn_model()\n",
    "    else:\n",
    "        model = build_cnn_model()\n",
    "        \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    history = model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=global_batch_size, epochs=global_epochs, shuffle=True, verbose=2)\n",
    "    model.save(fold_path + str(fold_index) + 'model_' + file_suffix + '.h5')\n",
    "\n",
    "    history_name = fold_path + str(fold_index) + 'history_' + file_suffix + '.history'\n",
    "\n",
    "    with open(history_name, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "\n",
    "    print(\"\\n\\ny_test : \", y_test)\n",
    "    print(\"\\n\\ny_prediction : \", y_prediction)\n",
    "\n",
    "    f1_scores = metrics.classification_report(y_test, y_prediction, digits=3)\n",
    "\n",
    "    original_scores = model.evaluate(old_X_test, old_y_test, verbose=1)\n",
    "    original_y_prediction = model.predict(old_X_test)\n",
    "    original_y_prediction = np.argmax(original_y_prediction, axis = 1)\n",
    "    original_f1_scores = metrics.classification_report(old_y_test, original_y_prediction, digits=3)\n",
    "\n",
    "    score_file = fold_path + str(fold_index) + 'score_' + file_suffix + '.txt'\n",
    "    with open(score_file, 'w') as f:\n",
    "        f.write('fold: ' + str(fold_index) + '\\n')\n",
    "        f.write('scores: ' + str(scores) + '\\n')\n",
    "        f.write('scores with original testdata: ' + str(original_scores) + '\\n')\n",
    "        f.write('f1_scores:\\n' + str(f1_scores) + '\\n')\n",
    "        f.write('f1_scores with original testdata:\\n' + str(original_f1_scores) + '\\n')\n",
    "\n",
    "    loss_file_path = fold_path + str(fold_index) + 'loss_' + file_suffix + '.png'\n",
    "    acc_file_path = fold_path + str(fold_index) + 'acc_' + file_suffix + '.png'\n",
    "    plot_acc_loss(history.history, acc_file_path, loss_file_path)\n",
    "\n",
    "    confusion_matrix_file_path = fold_path + str(fold_index) + 'confusion_matrix_' + file_suffix + '.png'\n",
    "    plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm(fold_index):\n",
    "    old_test_file = 'CustomDataFull/testdata2000.pkl'\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    old_X_test, old_y_test = get_old_test_data(old_test_file)\n",
    "\n",
    "    fold_path = 'results/lstm_results/' + global_folder_name + '/' + str(fold_index) + file_suffix + '/'\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "\n",
    "\n",
    "    if optimized_model:\n",
    "        model = build_optimized_lstm_model()\n",
    "    else:\n",
    "        model = build_lstm_model()\n",
    "\n",
    "        \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy'])\n",
    "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy', \n",
    "    #              tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.F1Score(num_classes=10, average='macro',\n",
    "    #              threshold=0.5)])\n",
    "    \n",
    "    model.summary()\n",
    "    history = model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=global_batch_size, epochs=global_epochs, shuffle=True, verbose=2)\n",
    "    model.save(fold_path + str(fold_index) + 'model_' + file_suffix + '.h5')\n",
    "\n",
    "    history_name = fold_path + str(fold_index) + 'history_' + file_suffix + '.history'\n",
    "\n",
    "    with open(history_name, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "    f1_scores = metrics.classification_report(y_test, y_prediction, digits=3)\n",
    "\n",
    "    original_scores = model.evaluate(old_X_test, old_y_test, verbose=1)\n",
    "    original_y_prediction = model.predict(old_X_test)\n",
    "    original_y_prediction = np.argmax(original_y_prediction, axis = 1)\n",
    "    original_f1_scores = metrics.classification_report(old_y_test, original_y_prediction, digits=3)\n",
    "    \n",
    "    score_file = fold_path + str(fold_index) + 'score_' + file_suffix + '.txt'\n",
    "    with open(score_file, 'w') as f:\n",
    "        f.write('fold: ' + str(fold_index) + '\\n')\n",
    "        f.write('scores: ' + str(scores) + '\\n')\n",
    "        f.write('scores with original testdata: ' + str(original_scores) + '\\n')\n",
    "        f.write('f1_scores: ' + str(f1_scores) + '\\n')\n",
    "        f.write('f1_scores with original testdata: ' + str(original_f1_scores) + '\\n')\n",
    "\n",
    "    loss_file_path = fold_path + str(fold_index) + 'loss_' + file_suffix + '.png'\n",
    "    acc_file_path = fold_path + str(fold_index) + 'acc_' + file_suffix + '.png'\n",
    "    plot_acc_loss(history.history, acc_file_path, loss_file_path)\n",
    "\n",
    "    confusion_matrix_file_path = fold_path + str(fold_index) + 'confusion_matrix_' + file_suffix + '.png'\n",
    "    plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_cnn():\n",
    "    \"\"\"\n",
    "    OBS! Efter en körning så printas summaryn över resultaten gällande de olika hyperparametrarna. Spara dom!\n",
    "    \"\"\"\n",
    "    tuner = keras_tuner.RandomSearch(hyperparameter_cnn_model, overwrite=True, objective='val_accuracy', max_trials=5, executions_per_trial=2, directory='hyperparameters', project_name=global_hyperparameter_folder_name)\n",
    "    tuner.search_space_summary()\n",
    "    fold_index = 0 # Godtyckligt val!!!\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    tuner.search(X_train, y_train, epochs=global_epochs, batch_size=global_batch_size, validation_data=(X_valid, y_valid), shuffle=True, verbose=2, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='hyperparameters/' + global_hyperparameter_folder_name)])\n",
    "    \n",
    "    print(tuner.get_best_hyperparameters()[0].values)\n",
    "\n",
    "    hyperparameter_log_name = 'hyperparameters/' + global_hyperparameter_folder_name + '/log.txt'\n",
    "\n",
    "    with open(hyperparameter_log_name, 'w') as f:\n",
    "        f.write('best hyperparameters: ' + str(tuner.get_best_hyperparameters()[0].values) + '\\n')\n",
    "        f.write(str(tuner.results_summary()) + '\\n')\n",
    "\n",
    "    tuner.results_summary()\n",
    "\n",
    "    copy_file = 'hyperparameters/' + global_hyperparameter_folder_name + '/' + 'cnn-hyperparameter_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lstm():\n",
    "    \"\"\"\n",
    "    OBS! Efter en körning så printas summaryn över resultaten gällande de olika hyperparametrarna. Spara dom!\n",
    "    \"\"\"\n",
    "    tuner = keras_tuner.RandomSearch(hyperparameter_lstm_model, overwrite=True, objective='val_accuracy', max_trials=5, executions_per_trial=2, directory='hyperparameters', project_name=global_hyperparameter_folder_name)\n",
    "    tuner.search_space_summary()\n",
    "    fold_index = 0 # Godtyckligt val!!!\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    tuner.search(X_train, y_train, epochs=global_epochs, batch_size=global_batch_size, validation_data=(X_valid, y_valid), shuffle=True, verbose=2, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='hyperparameters/' + global_hyperparameter_folder_name)])\n",
    "    \n",
    "    print(tuner.get_best_hyperparameters()[0].values)\n",
    "\n",
    "    hyperparameter_log_name = 'hyperparameters/' + global_hyperparameter_folder_name + '/log.txt'\n",
    "\n",
    "    with open(hyperparameter_log_name, 'w') as f:\n",
    "        f.write('best hyperparameters: ' + str(tuner.get_best_hyperparameters()[0].values) + '\\n')\n",
    "    \n",
    "    tuner.results_summary()\n",
    "    \n",
    "    copy_file = 'hyperparameters/' + global_hyperparameter_folder_name + '/' + 'lstm-hyperparameter_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_5_fold_cross_validation():\n",
    "    path = 'results/cnn_results/' + global_folder_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    copy_file = 'results/cnn_results/' + global_folder_name + '/' + 'copied_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        run_cnn(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_5_fold_cross_validation():\n",
    "    path = 'results/lstm_results/' + global_folder_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    copy_file = 'results/lstm_results/' + global_folder_name + '/' + 'copied_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        run_lstm(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running cnn\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 126, 13, 32)       320       \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 126, 13, 32)       9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 63, 6, 32)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 63, 6, 32)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 63, 6, 64)         18496     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 63, 6, 64)         36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 31, 3, 64)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 31, 3, 64)         0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 31, 3, 128)        73856     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 31, 3, 128)        0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 11904)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               6095360   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,239,338\n",
      "Trainable params: 6,239,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 02:13:48.089053: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/dropout_4/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-14 02:13:48.229900: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.230158: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.245061: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.245313: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.260502: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.260751: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.275700: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.275962: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.292413: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.292718: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.309132: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.309433: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.326118: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.326351: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.342553: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.342787: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.359187: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.359416: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.374674: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.374898: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.390874: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.391102: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.406026: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.406237: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.422481: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.422713: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.437996: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2023-05-14 02:13:48.438216: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_12' defined at (most recent call last):\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_8298/4270889375.py\", line 3, in <module>\n      run_cnn_5_fold_cross_validation()\n    File \"/tmp/ipykernel_8298/239300593.py\", line 11, in run_cnn_5_fold_cross_validation\n      run_cnn(i)\n    File \"/tmp/ipykernel_8298/2127468179.py\", line 25, in run_cnn\n      history = model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=global_batch_size, epochs=global_epochs, shuffle=True, verbose=2)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_12'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_12}}]] [Op:__inference_train_function_3809]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m (global_model_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcnn\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStart running cnn\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     run_cnn_5_fold_cross_validation()\n\u001b[1;32m      4\u001b[0m \u001b[39melif\u001b[39;00m (global_model_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlstm\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStart running lstm\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[40], line 11\u001b[0m, in \u001b[0;36mrun_cnn_5_fold_cross_validation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m shutil\u001b[39m.\u001b[39mcopyfile(own_file_name, copy_file)\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     run_cnn(i)\n",
      "Cell \u001b[0;32mIn[36], line 25\u001b[0m, in \u001b[0;36mrun_cnn\u001b[0;34m(fold_index)\u001b[0m\n\u001b[1;32m     23\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mglobal_learning_rate), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     24\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m---> 25\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mX_train, y\u001b[39m=\u001b[39;49my_train, validation_data\u001b[39m=\u001b[39;49m(X_valid, y_valid), batch_size\u001b[39m=\u001b[39;49mglobal_batch_size, epochs\u001b[39m=\u001b[39;49mglobal_epochs, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     26\u001b[0m model\u001b[39m.\u001b[39msave(fold_path \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(fold_index) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodel_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m file_suffix \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m history_name \u001b[39m=\u001b[39m fold_path \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(fold_index) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhistory_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m file_suffix \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.history\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_12' defined at (most recent call last):\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_8298/4270889375.py\", line 3, in <module>\n      run_cnn_5_fold_cross_validation()\n    File \"/tmp/ipykernel_8298/239300593.py\", line 11, in run_cnn_5_fold_cross_validation\n      run_cnn(i)\n    File \"/tmp/ipykernel_8298/2127468179.py\", line 25, in run_cnn\n      history = model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=global_batch_size, epochs=global_epochs, shuffle=True, verbose=2)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/stenmarken/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_12'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_12}}]] [Op:__inference_train_function_3809]"
     ]
    }
   ],
   "source": [
    "if (global_model_type == 'cnn'):\n",
    "    print('Start running cnn')\n",
    "    run_cnn_5_fold_cross_validation()\n",
    "elif (global_model_type == 'lstm'):\n",
    "    print('Start running lstm')\n",
    "    run_lstm_5_fold_cross_validation()\n",
    "elif (global_model_type == 'tune_cnn'):\n",
    "    print('Start tuning cnn')\n",
    "    tune_cnn()\n",
    "elif (global_model_type == 'tune_lstm'):\n",
    "    print('Start tuning lstm')\n",
    "    tune_lstm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
