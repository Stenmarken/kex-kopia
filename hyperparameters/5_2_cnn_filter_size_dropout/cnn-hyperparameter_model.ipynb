{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import time\n",
    "import collections\n",
    "import keras\n",
    "import random\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import MaxPooling1D, Conv1D, Conv2D, MaxPooling2D, GlobalAveragePooling2D, LSTM, ELU, Bidirectional, Attention\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, CuDNNLSTM\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras_tuner\n",
    "from keras import backend as K\n",
    "import shutil\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model_type = 'tune_cnn'\n",
    "global_folder_name = 'weyo4'\n",
    "file_suffix = 'weyo1'\n",
    "global_batch_size = 32\n",
    "global_epochs = 50\n",
    "global_learning_rate = 0.0001\n",
    "own_file_path = os.getcwd() \n",
    "global_hyperparameter_folder_name = '5_2_cnn_filter_size_dropout' #lstm_filter_size_dropout1' #'cnn_filter_size_dropout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instrument_code(filename):\n",
    "    \"\"\"\n",
    "    Function that takes in a filename and returns instrument based on naming convention\n",
    "    \"\"\"\n",
    "    # Synth lead borttagen. id = 9\n",
    "    class_names=['bass', 'brass', 'flute', 'guitar', \n",
    "             'keyboard', 'mallet', 'organ', 'reed', \n",
    "             'string', 'vocal']\n",
    "    \n",
    "    for name in class_names:\n",
    "        if name in filename:\n",
    "            return class_names.index(name)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        X_train_full  = pickle.load(f)\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for (key, value) in X_train_full:\n",
    "        X_train.append(value)\n",
    "        y_train.append(instrument_code(key))\n",
    "\n",
    "    X_train_numpy = np.array(X_train)\n",
    "    y_train_numpy = np.array(y_train)\n",
    "    return (X_train_numpy, y_train_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_old_test_data(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        old_X_test_full = pickle.load(f)\n",
    "\n",
    "    old_X_test = []\n",
    "    old_y_test = []\n",
    "\n",
    "    for(key, value) in old_X_test_full.items():\n",
    "        #temporal_value = np.mean(value, axis = 1)\n",
    "        old_X_test.append(value)\n",
    "        old_y_test.append(instrument_code(key))\n",
    "        \n",
    "    old_y_test_numpy = np.asarray(old_y_test)\n",
    "    old_X_test_numpy = np.asarray(old_X_test)\n",
    "    return (old_X_test_numpy, old_y_test_numpy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred):\n",
    "    print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_cnn_model(hp):\n",
    "    \"\"\"\n",
    "    Function that builds a CNN model with optimized hyperparameters including filter size and dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                    activation='relu', padding='same', input_shape=(126, 13, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_1', values = [32, 64, 128]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_1', values = [32, 64, 128]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_2', values = [32, 64, 128, 256]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=global_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_lstm_model(hp):\n",
    "    \"\"\"\n",
    "    Function that builds a LSTM model with optimized hyperparameters including filter size and dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    #model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True), input_shape=(126, 128)))\n",
    "    #model.add(CuDNNLSTM(4, input_shape=(126, 128), return_sequences=True))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, input_shape=(126, 13), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_1', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_1', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_2', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_2', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    #model.add(LSTM(16, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=global_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model():    \n",
    "    input_shape = (126, 13, 1)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128,(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    model = Sequential()\n",
    "    #model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True), input_shape=(126, 128)))\n",
    "    #model.add(CuDNNLSTM(4, input_shape=(126, 128), return_sequences=True))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, input_shape=(126, 13), return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    #model.add(LSTM(16, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(history, acc_file_path, loss_file_path):\n",
    "    # Taget från https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(acc_file_path)\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(loss_file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path):    \n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "    result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "\n",
    "    df_cm = pd.DataFrame(result, index = [i for i in [\"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\", \"mallet\", \"organ\", \"reed\", \"string\", \"vocal\"]],\n",
    "                    columns = [i for i in [\"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\", \"mallet\", \"organ\", \"reed\", \"string\", \"vocal\"]])\n",
    "\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(confusion_matrix_file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn(fold_index):\n",
    "\n",
    "    old_test_file = 'CustomDataFull/testdata2000.pkl'\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    old_X_test, old_y_test = get_old_test_data(old_test_file)\n",
    "\n",
    "    fold_path = 'results/cnn_results/' + global_folder_name + '/' + str(fold_index) + file_suffix + '/'\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "\n",
    "    model = build_cnn_model()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    history = model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=global_batch_size, epochs=global_epochs, shuffle=True, verbose=2)\n",
    "    model.save(fold_path + str(fold_index) + 'model_' + file_suffix + '.h5')\n",
    "\n",
    "    history_name = fold_path + str(fold_index) + 'history_' + file_suffix + '.history'\n",
    "\n",
    "    with open(history_name, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "\n",
    "    print(\"\\n\\ny_test : \", y_test)\n",
    "    print(\"\\n\\ny_prediction : \", y_prediction)\n",
    "\n",
    "    f1_scores = metrics.classification_report(y_test, y_prediction, digits=3)\n",
    "\n",
    "    original_scores = model.evaluate(old_X_test, old_y_test, verbose=1)\n",
    "    original_y_prediction = model.predict(old_X_test)\n",
    "    original_y_prediction = np.argmax(original_y_prediction, axis = 1)\n",
    "    original_f1_scores = metrics.classification_report(old_y_test, original_y_prediction, digits=3)\n",
    "\n",
    "    score_file = fold_path + str(fold_index) + 'score_' + file_suffix + '.txt'\n",
    "    with open(score_file, 'w') as f:\n",
    "        f.write('fold: ' + str(fold_index) + '\\n')\n",
    "        f.write('scores: ' + str(scores) + '\\n')\n",
    "        f.write('scores with original testdata: ' + str(original_scores) + '\\n')\n",
    "        f.write('f1_scores:\\n' + str(f1_scores) + '\\n')\n",
    "        f.write('f1_scores with original testdata:\\n' + str(original_f1_scores) + '\\n')\n",
    "\n",
    "    loss_file_path = fold_path + str(fold_index) + 'loss_' + file_suffix + '.png'\n",
    "    acc_file_path = fold_path + str(fold_index) + 'acc_' + file_suffix + '.png'\n",
    "    plot_acc_loss(history.history, acc_file_path, loss_file_path)\n",
    "\n",
    "    confusion_matrix_file_path = fold_path + str(fold_index) + 'confusion_matrix_' + file_suffix + '.png'\n",
    "    plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/async_helpers.py:129\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    130\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m exc\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3169\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3167\u001b[0m \u001b[39m# Store raw and processed history\u001b[39;00m\n\u001b[1;32m   3168\u001b[0m \u001b[39mif\u001b[39;00m store_history:\n\u001b[0;32m-> 3169\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory_manager\u001b[39m.\u001b[39;49mstore_inputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecution_count, cell, raw_cell)\n\u001b[1;32m   3170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m silent:\n\u001b[1;32m   3171\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog(cell, raw_cell)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/history.py:770\u001b[0m, in \u001b[0;36mHistoryManager.store_inputs\u001b[0;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_hist_parsed\u001b[39m.\u001b[39mappend(source)\n\u001b[1;32m    768\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_hist_raw\u001b[39m.\u001b[39mappend(source_raw)\n\u001b[0;32m--> 770\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdb_input_cache_lock:\n\u001b[1;32m    771\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdb_input_cache\u001b[39m.\u001b[39mappend((line_num, source, source_raw))\n\u001b[1;32m    772\u001b[0m     \u001b[39m# Trigger to flush cache and write to DB.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_lstm(fold_index):\n",
    "    old_test_file = 'CustomDataFull/testdata2000.pkl'\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    old_X_test, old_y_test = get_old_test_data(old_test_file)\n",
    "\n",
    "    fold_path = 'results/lstm_results/' + global_folder_name + '/' + str(fold_index) + file_suffix + '/'\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "\n",
    "    model = build_lstm_model()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy'])\n",
    "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy', \n",
    "    #              tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.F1Score(num_classes=10, average='macro',\n",
    "    #              threshold=0.5)])\n",
    "    \n",
    "    model.summary()\n",
    "    history = model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=global_batch_size, epochs=global_epochs, shuffle=True, verbose=2)\n",
    "    model.save(fold_path + str(fold_index) + 'model_' + file_suffix + '.h5')\n",
    "\n",
    "    history_name = fold_path + str(fold_index) + 'history_' + file_suffix + '.history'\n",
    "\n",
    "    with open(history_name, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "    f1_scores = metrics.classification_report(y_test, y_prediction, digits=3)\n",
    "\n",
    "    original_scores = model.evaluate(old_X_test, old_y_test, verbose=1)\n",
    "    original_y_prediction = model.predict(old_X_test)\n",
    "    original_y_prediction = np.argmax(original_y_prediction, axis = 1)\n",
    "    original_f1_scores = metrics.classification_report(old_y_test, original_y_prediction, digits=3)\n",
    "    \n",
    "    score_file = fold_path + str(fold_index) + 'score_' + file_suffix + '.txt'\n",
    "    with open(score_file, 'w') as f:\n",
    "        f.write('fold: ' + str(fold_index) + '\\n')\n",
    "        f.write('scores: ' + str(scores) + '\\n')\n",
    "        f.write('scores with original testdata: ' + str(original_scores) + '\\n')\n",
    "        f.write('f1_scores: ' + str(f1_scores) + '\\n')\n",
    "        f.write('f1_scores with original testdata: ' + str(original_f1_scores) + '\\n')\n",
    "\n",
    "    loss_file_path = fold_path + str(fold_index) + 'loss_' + file_suffix + '.png'\n",
    "    acc_file_path = fold_path + str(fold_index) + 'acc_' + file_suffix + '.png'\n",
    "    plot_acc_loss(history.history, acc_file_path, loss_file_path)\n",
    "\n",
    "    confusion_matrix_file_path = fold_path + str(fold_index) + 'confusion_matrix_' + file_suffix + '.png'\n",
    "    plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_cnn():\n",
    "    \"\"\"\n",
    "    OBS! Efter en körning så printas summaryn över resultaten gällande de olika hyperparametrarna. Spara dom!\n",
    "    \"\"\"\n",
    "    tuner = keras_tuner.RandomSearch(hyperparameter_cnn_model, overwrite=True, objective='val_accuracy', max_trials=5, executions_per_trial=2, directory='hyperparameters', project_name=global_hyperparameter_folder_name)\n",
    "    tuner.search_space_summary()\n",
    "    fold_index = 0 # Godtyckligt val!!!\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    tuner.search(X_train, y_train, epochs=global_epochs, batch_size=global_batch_size, validation_data=(X_valid, y_valid), shuffle=True, verbose=2, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='hyperparameters/' + global_hyperparameter_folder_name)])\n",
    "    \n",
    "    print(tuner.get_best_hyperparameters()[0].values)\n",
    "\n",
    "    hyperparameter_log_name = 'hyperparameters/' + global_hyperparameter_folder_name + '/log.txt'\n",
    "\n",
    "    with open(hyperparameter_log_name, 'w') as f:\n",
    "        f.write('best hyperparameters: ' + str(tuner.get_best_hyperparameters()[0].values) + '\\n')\n",
    "        f.write(str(tuner.results_summary()) + '\\n')\n",
    "\n",
    "    tuner.results_summary()\n",
    "\n",
    "    copy_file = 'hyperparameters/' + global_hyperparameter_folder_name + '/' + 'cnn-hyperparameter_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lstm():\n",
    "    \"\"\"\n",
    "    OBS! Efter en körning så printas summaryn över resultaten gällande de olika hyperparametrarna. Spara dom!\n",
    "    \"\"\"\n",
    "    tuner = keras_tuner.RandomSearch(hyperparameter_lstm_model, overwrite=True, objective='val_accuracy', max_trials=5, executions_per_trial=2, directory='hyperparameters', project_name=global_hyperparameter_folder_name)\n",
    "    tuner.search_space_summary()\n",
    "    fold_index = 0 # Godtyckligt val!!!\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    tuner.search(X_train, y_train, epochs=global_epochs, batch_size=global_batch_size, validation_data=(X_valid, y_valid), shuffle=True, verbose=2, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='hyperparameters/' + global_hyperparameter_folder_name)])\n",
    "    \n",
    "    print(tuner.get_best_hyperparameters()[0].values)\n",
    "\n",
    "    hyperparameter_log_name = 'hyperparameters/' + global_hyperparameter_folder_name + '/log.txt'\n",
    "\n",
    "    with open(hyperparameter_log_name, 'w') as f:\n",
    "        f.write('best hyperparameters: ' + str(tuner.get_best_hyperparameters()[0].values) + '\\n')\n",
    "    \n",
    "    tuner.results_summary()\n",
    "    \n",
    "    copy_file = 'hyperparameters/' + global_hyperparameter_folder_name + '/' + 'lstm-hyperparameter_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_5_fold_cross_validation():\n",
    "    path = 'results/cnn_results/' + global_folder_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    copy_file = 'results/cnn_results/' + global_folder_name + '/' + 'copied_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        run_cnn(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_5_fold_cross_validation():\n",
    "    path = 'results/lstm_results/' + global_folder_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    copy_file = 'results/lstm_results/' + global_folder_name + '/' + 'copied_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        run_lstm(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 30m 50s]\n",
      "val_accuracy: 0.9370285868644714\n",
      "\n",
      "Best val_accuracy So Far: 0.9370285868644714\n",
      "Total elapsed time: 01h 00m 45s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.4               |0.5               |dropout_1\n",
      "128               |128               |conv_2_filters_1\n",
      "256               |32                |conv_2_filters_2\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 21:06:55.286177: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_train_batch_end` time: 0.0118s). Check your callbacks.\n",
      "2188/2188 - 28s - loss: 1.6905 - accuracy: 0.3761 - val_loss: 1.2445 - val_accuracy: 0.5395 - 28s/epoch - 13ms/step\n",
      "Epoch 2/50\n",
      "2188/2188 - 27s - loss: 1.1498 - accuracy: 0.5845 - val_loss: 0.9748 - val_accuracy: 0.6419 - 27s/epoch - 12ms/step\n",
      "Epoch 3/50\n",
      "2188/2188 - 27s - loss: 0.9540 - accuracy: 0.6534 - val_loss: 0.7600 - val_accuracy: 0.7383 - 27s/epoch - 12ms/step\n",
      "Epoch 4/50\n",
      "2188/2188 - 27s - loss: 0.8313 - accuracy: 0.6994 - val_loss: 0.6658 - val_accuracy: 0.7645 - 27s/epoch - 12ms/step\n",
      "Epoch 5/50\n",
      "2188/2188 - 27s - loss: 0.7426 - accuracy: 0.7325 - val_loss: 0.5849 - val_accuracy: 0.7922 - 27s/epoch - 12ms/step\n",
      "Epoch 6/50\n",
      "2188/2188 - 27s - loss: 0.6749 - accuracy: 0.7564 - val_loss: 0.5578 - val_accuracy: 0.7967 - 27s/epoch - 12ms/step\n",
      "Epoch 7/50\n",
      "2188/2188 - 27s - loss: 0.6173 - accuracy: 0.7763 - val_loss: 0.4797 - val_accuracy: 0.8295 - 27s/epoch - 12ms/step\n",
      "Epoch 8/50\n",
      "2188/2188 - 27s - loss: 0.5723 - accuracy: 0.7935 - val_loss: 0.4144 - val_accuracy: 0.8538 - 27s/epoch - 12ms/step\n",
      "Epoch 9/50\n",
      "2188/2188 - 27s - loss: 0.5298 - accuracy: 0.8079 - val_loss: 0.3974 - val_accuracy: 0.8554 - 27s/epoch - 12ms/step\n",
      "Epoch 10/50\n",
      "2188/2188 - 27s - loss: 0.4956 - accuracy: 0.8191 - val_loss: 0.3481 - val_accuracy: 0.8735 - 27s/epoch - 12ms/step\n",
      "Epoch 11/50\n",
      "2188/2188 - 27s - loss: 0.4626 - accuracy: 0.8332 - val_loss: 0.3354 - val_accuracy: 0.8783 - 27s/epoch - 12ms/step\n",
      "Epoch 12/50\n",
      "2188/2188 - 26s - loss: 0.4404 - accuracy: 0.8387 - val_loss: 0.3341 - val_accuracy: 0.8827 - 26s/epoch - 12ms/step\n",
      "Epoch 13/50\n",
      "2188/2188 - 27s - loss: 0.4176 - accuracy: 0.8487 - val_loss: 0.2879 - val_accuracy: 0.8942 - 27s/epoch - 12ms/step\n",
      "Epoch 14/50\n",
      "2188/2188 - 27s - loss: 0.3948 - accuracy: 0.8567 - val_loss: 0.2740 - val_accuracy: 0.9013 - 27s/epoch - 12ms/step\n",
      "Epoch 15/50\n",
      "2188/2188 - 27s - loss: 0.3810 - accuracy: 0.8608 - val_loss: 0.2524 - val_accuracy: 0.9114 - 27s/epoch - 12ms/step\n",
      "Epoch 16/50\n",
      "2188/2188 - 25s - loss: 0.3597 - accuracy: 0.8689 - val_loss: 0.2472 - val_accuracy: 0.9107 - 25s/epoch - 12ms/step\n",
      "Epoch 17/50\n",
      "2188/2188 - 26s - loss: 0.3398 - accuracy: 0.8758 - val_loss: 0.2290 - val_accuracy: 0.9205 - 26s/epoch - 12ms/step\n",
      "Epoch 18/50\n",
      "2188/2188 - 27s - loss: 0.3290 - accuracy: 0.8803 - val_loss: 0.2281 - val_accuracy: 0.9215 - 27s/epoch - 12ms/step\n",
      "Epoch 19/50\n",
      "2188/2188 - 25s - loss: 0.3176 - accuracy: 0.8849 - val_loss: 0.2212 - val_accuracy: 0.9214 - 25s/epoch - 12ms/step\n",
      "Epoch 20/50\n",
      "2188/2188 - 26s - loss: 0.3067 - accuracy: 0.8867 - val_loss: 0.1954 - val_accuracy: 0.9330 - 26s/epoch - 12ms/step\n",
      "Epoch 21/50\n",
      "2188/2188 - 25s - loss: 0.2934 - accuracy: 0.8930 - val_loss: 0.1970 - val_accuracy: 0.9303 - 25s/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "2188/2188 - 27s - loss: 0.2811 - accuracy: 0.8961 - val_loss: 0.1861 - val_accuracy: 0.9359 - 27s/epoch - 12ms/step\n",
      "Epoch 23/50\n",
      "2188/2188 - 26s - loss: 0.2721 - accuracy: 0.9006 - val_loss: 0.1761 - val_accuracy: 0.9367 - 26s/epoch - 12ms/step\n",
      "Epoch 24/50\n",
      "2188/2188 - 27s - loss: 0.2612 - accuracy: 0.9050 - val_loss: 0.1760 - val_accuracy: 0.9371 - 27s/epoch - 12ms/step\n",
      "Epoch 25/50\n",
      "2188/2188 - 25s - loss: 0.2550 - accuracy: 0.9069 - val_loss: 0.1945 - val_accuracy: 0.9319 - 25s/epoch - 12ms/step\n",
      "Epoch 26/50\n",
      "2188/2188 - 25s - loss: 0.2456 - accuracy: 0.9104 - val_loss: 0.2237 - val_accuracy: 0.9200 - 25s/epoch - 12ms/step\n",
      "Epoch 27/50\n",
      "2188/2188 - 25s - loss: 0.2379 - accuracy: 0.9130 - val_loss: 0.1782 - val_accuracy: 0.9359 - 25s/epoch - 12ms/step\n",
      "Epoch 28/50\n",
      "2188/2188 - 27s - loss: 0.2299 - accuracy: 0.9157 - val_loss: 0.1496 - val_accuracy: 0.9480 - 27s/epoch - 12ms/step\n",
      "Epoch 29/50\n",
      "2188/2188 - 27s - loss: 0.2228 - accuracy: 0.9194 - val_loss: 0.1458 - val_accuracy: 0.9489 - 27s/epoch - 12ms/step\n",
      "Epoch 30/50\n",
      "2188/2188 - 27s - loss: 0.2182 - accuracy: 0.9207 - val_loss: 0.1428 - val_accuracy: 0.9513 - 27s/epoch - 12ms/step\n",
      "Epoch 31/50\n",
      "2188/2188 - 25s - loss: 0.2103 - accuracy: 0.9219 - val_loss: 0.1504 - val_accuracy: 0.9488 - 25s/epoch - 12ms/step\n",
      "Epoch 32/50\n",
      "2188/2188 - 26s - loss: 0.2016 - accuracy: 0.9269 - val_loss: 0.1311 - val_accuracy: 0.9546 - 26s/epoch - 12ms/step\n",
      "Epoch 33/50\n",
      "2188/2188 - 27s - loss: 0.1977 - accuracy: 0.9269 - val_loss: 0.1245 - val_accuracy: 0.9578 - 27s/epoch - 12ms/step\n",
      "Epoch 34/50\n",
      "2188/2188 - 27s - loss: 0.1963 - accuracy: 0.9282 - val_loss: 0.1223 - val_accuracy: 0.9583 - 27s/epoch - 12ms/step\n",
      "Epoch 35/50\n",
      "2188/2188 - 27s - loss: 0.1865 - accuracy: 0.9325 - val_loss: 0.1210 - val_accuracy: 0.9593 - 27s/epoch - 12ms/step\n",
      "Epoch 36/50\n",
      "2188/2188 - 27s - loss: 0.1853 - accuracy: 0.9326 - val_loss: 0.1134 - val_accuracy: 0.9625 - 27s/epoch - 12ms/step\n",
      "Epoch 37/50\n",
      "2188/2188 - 25s - loss: 0.1750 - accuracy: 0.9358 - val_loss: 0.1163 - val_accuracy: 0.9597 - 25s/epoch - 12ms/step\n",
      "Epoch 38/50\n",
      "2188/2188 - 25s - loss: 0.1753 - accuracy: 0.9352 - val_loss: 0.1184 - val_accuracy: 0.9590 - 25s/epoch - 12ms/step\n",
      "Epoch 39/50\n",
      "2188/2188 - 27s - loss: 0.1692 - accuracy: 0.9379 - val_loss: 0.1084 - val_accuracy: 0.9641 - 27s/epoch - 12ms/step\n",
      "Epoch 40/50\n",
      "2188/2188 - 25s - loss: 0.1661 - accuracy: 0.9394 - val_loss: 0.1182 - val_accuracy: 0.9595 - 25s/epoch - 12ms/step\n",
      "Epoch 41/50\n",
      "2188/2188 - 25s - loss: 0.1613 - accuracy: 0.9407 - val_loss: 0.1071 - val_accuracy: 0.9633 - 25s/epoch - 12ms/step\n",
      "Epoch 42/50\n",
      "2188/2188 - 27s - loss: 0.1584 - accuracy: 0.9423 - val_loss: 0.0985 - val_accuracy: 0.9666 - 27s/epoch - 12ms/step\n",
      "Epoch 43/50\n",
      "2188/2188 - 25s - loss: 0.1562 - accuracy: 0.9436 - val_loss: 0.0992 - val_accuracy: 0.9654 - 25s/epoch - 12ms/step\n",
      "Epoch 44/50\n",
      "2188/2188 - 25s - loss: 0.1484 - accuracy: 0.9459 - val_loss: 0.1014 - val_accuracy: 0.9656 - 25s/epoch - 12ms/step\n",
      "Epoch 45/50\n",
      "2188/2188 - 25s - loss: 0.1476 - accuracy: 0.9463 - val_loss: 0.1038 - val_accuracy: 0.9666 - 25s/epoch - 12ms/step\n",
      "Epoch 46/50\n",
      "2188/2188 - 25s - loss: 0.1430 - accuracy: 0.9481 - val_loss: 0.1073 - val_accuracy: 0.9653 - 25s/epoch - 12ms/step\n",
      "Epoch 47/50\n",
      "2188/2188 - 26s - loss: 0.1408 - accuracy: 0.9488 - val_loss: 0.0965 - val_accuracy: 0.9679 - 26s/epoch - 12ms/step\n",
      "Epoch 48/50\n",
      "2188/2188 - 25s - loss: 0.1376 - accuracy: 0.9494 - val_loss: 0.0995 - val_accuracy: 0.9665 - 25s/epoch - 12ms/step\n",
      "Epoch 49/50\n",
      "2188/2188 - 27s - loss: 0.1353 - accuracy: 0.9506 - val_loss: 0.0917 - val_accuracy: 0.9698 - 27s/epoch - 12ms/step\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39melif\u001b[39;00m (global_model_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtune_cnn\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStart tuning cnn\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     tune_cnn()\n\u001b[1;32m     10\u001b[0m \u001b[39melif\u001b[39;00m (global_model_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtune_lstm\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStart tuning lstm\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[36], line 16\u001b[0m, in \u001b[0;36mtune_cnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m X_test, y_test \u001b[39m=\u001b[39m get_data_from_file(test_file)\n\u001b[1;32m     15\u001b[0m X_valid, y_valid \u001b[39m=\u001b[39m get_data_from_file(valid_file)\n\u001b[0;32m---> 16\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(X_train, y_train, epochs\u001b[39m=\u001b[39;49mglobal_epochs, batch_size\u001b[39m=\u001b[39;49mglobal_batch_size, validation_data\u001b[39m=\u001b[39;49m(X_valid, y_valid), shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mTensorBoard(log_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhyperparameters/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m global_hyperparameter_folder_name)])\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(tuner\u001b[39m.\u001b[39mget_best_hyperparameters()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m     20\u001b[0m hyperparameter_log_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhyperparameters/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m global_hyperparameter_folder_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/log.txt\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:230\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 230\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    231\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:270\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[1;32m    269\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    271\u001b[0m         trial\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m trial_module\u001b[39m.\u001b[39mTrialStatus\u001b[39m.\u001b[39mCOMPLETED\n\u001b[1;32m    272\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 235\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_trial(trial\u001b[39m.\u001b[39mtrial_id)\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mexists(\n\u001b[1;32m    237\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective\u001b[39m.\u001b[39mname\n\u001b[1;32m    238\u001b[0m     ):\n\u001b[1;32m    239\u001b[0m         \u001b[39m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    240\u001b[0m         \u001b[39m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    241\u001b[0m         \u001b[39m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    243\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe use case of calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    251\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras_tuner/engine/tuner.py:287\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    286\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[0;32m--> 287\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_and_fit_model(trial, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcopied_kwargs)\n\u001b[1;32m    289\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[1;32m    290\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras_tuner/engine/tuner.py:214\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m hp \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mhyperparameters\n\u001b[1;32m    213\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 214\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhypermodel\u001b[39m.\u001b[39;49mfit(hp, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m tuner_utils\u001b[39m.\u001b[39mvalidate_trial_results(\n\u001b[1;32m    216\u001b[0m     results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective, \u001b[39m\"\u001b[39m\u001b[39mHyperModel.fit()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras_tuner/engine/hypermodel.py:144\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py:1656\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1654\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1655\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1656\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1657\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1658\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/callbacks.py:476\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 476\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/callbacks.py:323\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    322\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    326\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/callbacks.py:346\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    344\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 346\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    349\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/callbacks.py:391\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    389\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 391\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_logs(logs, is_batch_hook\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    392\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    393\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/callbacks.py:298\u001b[0m, in \u001b[0;36mCallbackList._process_logs\u001b[0;34m(self, logs, is_batch_hook)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m is_batch_hook \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_hooks_support_tf_logs:\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m logs\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/tf_utils.py:665\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    663\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 665\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/tf_utils.py:658\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    656\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    657\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 658\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    659\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1155\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \n\u001b[1;32m   1134\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1155\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1121\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1120\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1122\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if (global_model_type == 'cnn'):\n",
    "    print('Start running cnn')\n",
    "    run_cnn_5_fold_cross_validation()\n",
    "elif (global_model_type == 'lstm'):\n",
    "    print('Start running lstm')\n",
    "    run_lstm_5_fold_cross_validation()\n",
    "elif (global_model_type == 'tune_cnn'):\n",
    "    print('Start tuning cnn')\n",
    "    tune_cnn()\n",
    "elif (global_model_type == 'tune_lstm'):\n",
    "    print('Start tuning lstm')\n",
    "    tune_lstm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
