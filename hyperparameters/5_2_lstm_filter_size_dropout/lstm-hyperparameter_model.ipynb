{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import time\n",
    "import collections\n",
    "import keras\n",
    "import random\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import MaxPooling1D, Conv1D, Conv2D, MaxPooling2D, GlobalAveragePooling2D, LSTM, ELU, Bidirectional, Attention\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, CuDNNLSTM\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras_tuner\n",
    "from keras import backend as K\n",
    "import shutil\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model_type = 'tune_lstm'\n",
    "global_folder_name = 'weyo4'\n",
    "file_suffix = 'weyo1'\n",
    "global_batch_size = 32\n",
    "global_epochs = 50\n",
    "global_learning_rate = 0.001\n",
    "own_file_path = os.getcwd() \n",
    "global_hyperparameter_folder_name = '5_2_lstm_filter_size_dropout' #lstm_filter_size_dropout1' #'cnn_filter_size_dropout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instrument_code(filename):\n",
    "    \"\"\"\n",
    "    Function that takes in a filename and returns instrument based on naming convention\n",
    "    \"\"\"\n",
    "    # Synth lead borttagen. id = 9\n",
    "    class_names=['bass', 'brass', 'flute', 'guitar', \n",
    "             'keyboard', 'mallet', 'organ', 'reed', \n",
    "             'string', 'vocal']\n",
    "    \n",
    "    for name in class_names:\n",
    "        if name in filename:\n",
    "            return class_names.index(name)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        X_train_full  = pickle.load(f)\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for (key, value) in X_train_full:\n",
    "        X_train.append(value)\n",
    "        y_train.append(instrument_code(key))\n",
    "\n",
    "    X_train_numpy = np.array(X_train)\n",
    "    y_train_numpy = np.array(y_train)\n",
    "    return (X_train_numpy, y_train_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_old_test_data(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        old_X_test_full = pickle.load(f)\n",
    "\n",
    "    old_X_test = []\n",
    "    old_y_test = []\n",
    "\n",
    "    for(key, value) in old_X_test_full.items():\n",
    "        #temporal_value = np.mean(value, axis = 1)\n",
    "        old_X_test.append(value)\n",
    "        old_y_test.append(instrument_code(key))\n",
    "        \n",
    "    old_y_test_numpy = np.asarray(old_y_test)\n",
    "    old_X_test_numpy = np.asarray(old_X_test)\n",
    "    return (old_X_test_numpy, old_y_test_numpy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred):\n",
    "    print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_cnn_model(hp):\n",
    "    \"\"\"\n",
    "    Function that builds a CNN model with optimized hyperparameters including filter size and dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                    activation='relu', padding='same', input_shape=(126, 13, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_1', values = [32, 64, 128]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_1', values = [32, 64, 128]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Conv2D(filters = hp.Choice('conv_2_filters_2', values = [32, 64, 128, 256]), kernel_size=(3, 3),\n",
    "                    activation='relu', padding='same'))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=global_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_lstm_model(hp):\n",
    "    \"\"\"\n",
    "    Function that builds a LSTM model with optimized hyperparameters including filter size and dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    #model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True), input_shape=(126, 128)))\n",
    "    #model.add(CuDNNLSTM(4, input_shape=(126, 128), return_sequences=True))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, input_shape=(126, 13), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_1', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_1', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_2', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units = hp.Choice('lstm_layer_units_2', values = [32, 64, 128]), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_lstm', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    #model.add(LSTM(16, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=global_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model():    \n",
    "    input_shape = (126, 13, 1)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128,(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    model = Sequential()\n",
    "    #model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True), input_shape=(126, 128)))\n",
    "    #model.add(CuDNNLSTM(4, input_shape=(126, 128), return_sequences=True))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, input_shape=(126, 13), return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    #model.add(LSTM(16, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(history, acc_file_path, loss_file_path):\n",
    "    # Taget från https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(acc_file_path)\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(loss_file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path):    \n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "    result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "\n",
    "    df_cm = pd.DataFrame(result, index = [i for i in [\"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\", \"mallet\", \"organ\", \"reed\", \"string\", \"vocal\"]],\n",
    "                    columns = [i for i in [\"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\", \"mallet\", \"organ\", \"reed\", \"string\", \"vocal\"]])\n",
    "\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(confusion_matrix_file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn(fold_index):\n",
    "\n",
    "    old_test_file = 'CustomDataFull/testdata2000.pkl'\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    old_X_test, old_y_test = get_old_test_data(old_test_file)\n",
    "\n",
    "    fold_path = 'results/cnn_results/' + global_folder_name + '/' + str(fold_index) + file_suffix + '/'\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "\n",
    "    model = build_cnn_model()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    history = model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=global_batch_size, epochs=global_epochs, shuffle=True, verbose=2)\n",
    "    model.save(fold_path + str(fold_index) + 'model_' + file_suffix + '.h5')\n",
    "\n",
    "    history_name = fold_path + str(fold_index) + 'history_' + file_suffix + '.history'\n",
    "\n",
    "    with open(history_name, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "\n",
    "    print(\"\\n\\ny_test : \", y_test)\n",
    "    print(\"\\n\\ny_prediction : \", y_prediction)\n",
    "\n",
    "    f1_scores = metrics.classification_report(y_test, y_prediction, digits=3)\n",
    "\n",
    "    original_scores = model.evaluate(old_X_test, old_y_test, verbose=1)\n",
    "    original_y_prediction = model.predict(old_X_test)\n",
    "    original_y_prediction = np.argmax(original_y_prediction, axis = 1)\n",
    "    original_f1_scores = metrics.classification_report(old_y_test, original_y_prediction, digits=3)\n",
    "\n",
    "    score_file = fold_path + str(fold_index) + 'score_' + file_suffix + '.txt'\n",
    "    with open(score_file, 'w') as f:\n",
    "        f.write('fold: ' + str(fold_index) + '\\n')\n",
    "        f.write('scores: ' + str(scores) + '\\n')\n",
    "        f.write('scores with original testdata: ' + str(original_scores) + '\\n')\n",
    "        f.write('f1_scores:\\n' + str(f1_scores) + '\\n')\n",
    "        f.write('f1_scores with original testdata:\\n' + str(original_f1_scores) + '\\n')\n",
    "\n",
    "    loss_file_path = fold_path + str(fold_index) + 'loss_' + file_suffix + '.png'\n",
    "    acc_file_path = fold_path + str(fold_index) + 'acc_' + file_suffix + '.png'\n",
    "    plot_acc_loss(history.history, acc_file_path, loss_file_path)\n",
    "\n",
    "    confusion_matrix_file_path = fold_path + str(fold_index) + 'confusion_matrix_' + file_suffix + '.png'\n",
    "    plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm(fold_index):\n",
    "    old_test_file = 'CustomDataFull/testdata2000.pkl'\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    old_X_test, old_y_test = get_old_test_data(old_test_file)\n",
    "\n",
    "    fold_path = 'results/lstm_results/' + global_folder_name + '/' + str(fold_index) + file_suffix + '/'\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "\n",
    "    model = build_lstm_model()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy'])\n",
    "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=global_learning_rate), metrics=['accuracy', \n",
    "    #              tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.F1Score(num_classes=10, average='macro',\n",
    "    #              threshold=0.5)])\n",
    "    \n",
    "    model.summary()\n",
    "    history = model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=global_batch_size, epochs=global_epochs, shuffle=True, verbose=2)\n",
    "    model.save(fold_path + str(fold_index) + 'model_' + file_suffix + '.h5')\n",
    "\n",
    "    history_name = fold_path + str(fold_index) + 'history_' + file_suffix + '.history'\n",
    "\n",
    "    with open(history_name, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    y_prediction = model.predict(X_test)\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "    f1_scores = metrics.classification_report(y_test, y_prediction, digits=3)\n",
    "\n",
    "    original_scores = model.evaluate(old_X_test, old_y_test, verbose=1)\n",
    "    original_y_prediction = model.predict(old_X_test)\n",
    "    original_y_prediction = np.argmax(original_y_prediction, axis = 1)\n",
    "    original_f1_scores = metrics.classification_report(old_y_test, original_y_prediction, digits=3)\n",
    "    \n",
    "    score_file = fold_path + str(fold_index) + 'score_' + file_suffix + '.txt'\n",
    "    with open(score_file, 'w') as f:\n",
    "        f.write('fold: ' + str(fold_index) + '\\n')\n",
    "        f.write('scores: ' + str(scores) + '\\n')\n",
    "        f.write('scores with original testdata: ' + str(original_scores) + '\\n')\n",
    "        f.write('f1_scores: ' + str(f1_scores) + '\\n')\n",
    "        f.write('f1_scores with original testdata: ' + str(original_f1_scores) + '\\n')\n",
    "\n",
    "    loss_file_path = fold_path + str(fold_index) + 'loss_' + file_suffix + '.png'\n",
    "    acc_file_path = fold_path + str(fold_index) + 'acc_' + file_suffix + '.png'\n",
    "    plot_acc_loss(history.history, acc_file_path, loss_file_path)\n",
    "\n",
    "    confusion_matrix_file_path = fold_path + str(fold_index) + 'confusion_matrix_' + file_suffix + '.png'\n",
    "    plt_confusion_matrix(model, X_test, y_test, confusion_matrix_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_cnn():\n",
    "    \"\"\"\n",
    "    OBS! Efter en körning så printas summaryn över resultaten gällande de olika hyperparametrarna. Spara dom!\n",
    "    \"\"\"\n",
    "    tuner = keras_tuner.RandomSearch(hyperparameter_cnn_model, overwrite=True, objective='val_accuracy', max_trials=5, executions_per_trial=2, directory='hyperparameters', project_name=global_hyperparameter_folder_name)\n",
    "    tuner.search_space_summary()\n",
    "    fold_index = 0 # Godtyckligt val!!!\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    tuner.search(X_train, y_train, epochs=global_epochs, batch_size=global_batch_size, validation_data=(X_valid, y_valid), shuffle=True, verbose=2, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='hyperparameters/' + global_hyperparameter_folder_name)])\n",
    "    \n",
    "    print(tuner.get_best_hyperparameters()[0].values)\n",
    "\n",
    "    hyperparameter_log_name = 'hyperparameters/' + global_hyperparameter_folder_name + '/log.txt'\n",
    "\n",
    "    with open(hyperparameter_log_name, 'w') as f:\n",
    "        f.write('best hyperparameters: ' + str(tuner.get_best_hyperparameters()[0].values) + '\\n')\n",
    "        f.write(str(tuner.results_summary()) + '\\n')\n",
    "\n",
    "    tuner.results_summary()\n",
    "\n",
    "    copy_file = 'hyperparameters/' + global_hyperparameter_folder_name + '/' + 'cnn-hyperparameter_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lstm():\n",
    "    \"\"\"\n",
    "    OBS! Efter en körning så printas summaryn över resultaten gällande de olika hyperparametrarna. Spara dom!\n",
    "    \"\"\"\n",
    "    tuner = keras_tuner.RandomSearch(hyperparameter_lstm_model, overwrite=True, objective='val_accuracy', max_trials=5, executions_per_trial=2, directory='hyperparameters', project_name=global_hyperparameter_folder_name)\n",
    "    tuner.search_space_summary()\n",
    "    fold_index = 0 # Godtyckligt val!!!\n",
    "\n",
    "    train_file = 'folds/' + str(fold_index) + 'train_' + file_suffix\n",
    "    test_file = 'folds/' + str(fold_index) + 'test_' + file_suffix\n",
    "    valid_file = 'folds/' + str(fold_index) + 'valid_' + file_suffix\n",
    "\n",
    "    X_train, y_train = get_data_from_file(train_file)\n",
    "    X_test, y_test = get_data_from_file(test_file)\n",
    "    X_valid, y_valid = get_data_from_file(valid_file)\n",
    "    tuner.search(X_train, y_train, epochs=global_epochs, batch_size=global_batch_size, validation_data=(X_valid, y_valid), shuffle=True, verbose=2, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='hyperparameters/' + global_hyperparameter_folder_name)])\n",
    "    \n",
    "    print(tuner.get_best_hyperparameters()[0].values)\n",
    "\n",
    "    hyperparameter_log_name = 'hyperparameters/' + global_hyperparameter_folder_name + '/log.txt'\n",
    "\n",
    "    with open(hyperparameter_log_name, 'w') as f:\n",
    "        f.write('best hyperparameters: ' + str(tuner.get_best_hyperparameters()[0].values) + '\\n')\n",
    "    \n",
    "    tuner.results_summary()\n",
    "    \n",
    "    copy_file = 'hyperparameters/' + global_hyperparameter_folder_name + '/' + 'lstm-hyperparameter_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_5_fold_cross_validation():\n",
    "    path = 'results/cnn_results/' + global_folder_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    copy_file = 'results/cnn_results/' + global_folder_name + '/' + 'copied_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        run_cnn(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_5_fold_cross_validation():\n",
    "    path = 'results/lstm_results/' + global_folder_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    copy_file = 'results/lstm_results/' + global_folder_name + '/' + 'copied_model.ipynb'\n",
    "    own_file_name = own_file_path + '/k-fold_cross_validation.ipynb'\n",
    "    shutil.copyfile(own_file_name, copy_file)\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        run_lstm(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tuning lstm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39melif\u001b[39;00m (global_model_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtune_lstm\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStart tuning lstm\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     tune_lstm()\n",
      "Cell \u001b[0;32mIn[88], line 5\u001b[0m, in \u001b[0;36mtune_lstm\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_lstm\u001b[39m():\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    OBS! Efter en körning så printas summaryn över resultaten gällande de olika hyperparametrarna. Spara dom!\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     tuner \u001b[39m=\u001b[39m keras_tuner\u001b[39m.\u001b[39;49mRandomSearch(hyperparameter_lstm_model, overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, objective\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m, max_trials\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, executions_per_trial\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhyperparameters\u001b[39;49m\u001b[39m'\u001b[39;49m, project_name\u001b[39m=\u001b[39;49mglobal_hyperparameter_folder_name)\n\u001b[1;32m      6\u001b[0m     tuner\u001b[39m.\u001b[39msearch_space_summary()\n\u001b[1;32m      7\u001b[0m     fold_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# Godtyckligt val!!!\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras_tuner/tuners/randomsearch.py:174\u001b[0m, in \u001b[0;36mRandomSearch.__init__\u001b[0;34m(self, hypermodel, objective, max_trials, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed \u001b[39m=\u001b[39m seed\n\u001b[1;32m    164\u001b[0m oracle \u001b[39m=\u001b[39m RandomSearchOracle(\n\u001b[1;32m    165\u001b[0m     objective\u001b[39m=\u001b[39mobjective,\n\u001b[1;32m    166\u001b[0m     max_trials\u001b[39m=\u001b[39mmax_trials,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m     max_consecutive_failed_trials\u001b[39m=\u001b[39mmax_consecutive_failed_trials,\n\u001b[1;32m    173\u001b[0m )\n\u001b[0;32m--> 174\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(oracle, hypermodel, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras_tuner/engine/tuner.py:113\u001b[0m, in \u001b[0;36mTuner.__init__\u001b[0;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m hypermodel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39mrun_trial \u001b[39mis\u001b[39;00m Tuner\u001b[39m.\u001b[39mrun_trial:\n\u001b[1;32m    106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mReceived `hypermodel=None`. We only allow not specifying \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`hypermodel` if the user defines the search space in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`Tuner.run_trial()` by subclassing a `Tuner` class without \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39musing a `HyperModel` instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m     )\n\u001b[0;32m--> 113\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    114\u001b[0m     oracle\u001b[39m=\u001b[39;49moracle,\n\u001b[1;32m    115\u001b[0m     hypermodel\u001b[39m=\u001b[39;49mhypermodel,\n\u001b[1;32m    116\u001b[0m     directory\u001b[39m=\u001b[39;49mdirectory,\n\u001b[1;32m    117\u001b[0m     project_name\u001b[39m=\u001b[39;49mproject_name,\n\u001b[1;32m    118\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[1;32m    119\u001b[0m     overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[1;32m    120\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_model_size \u001b[39m=\u001b[39m max_model_size\n\u001b[1;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m optimizer\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:133\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, oracle, hypermodel, directory, project_name, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreload()\n\u001b[1;32m    131\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     \u001b[39m# Only populate initial space if not reloading.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_populate_initial_space()\n\u001b[1;32m    135\u001b[0m \u001b[39m# Run in distributed mode.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m dist_utils\u001b[39m.\u001b[39mis_chief_oracle():\n\u001b[1;32m    137\u001b[0m     \u001b[39m# Blocks forever.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[39m# Avoid import at the top, to avoid inconsistent protobuf versions.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:204\u001b[0m, in \u001b[0;36mBaseTuner._populate_initial_space\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypermodel\u001b[39m.\u001b[39mdeclare_hyperparameters(hp)\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mupdate_space(hp)\n\u001b[0;32m--> 204\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_activate_all_conditions()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:161\u001b[0m, in \u001b[0;36mBaseTuner._activate_all_conditions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m hp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_space()\n\u001b[1;32m    160\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhypermodel\u001b[39m.\u001b[39;49mbuild(hp)\n\u001b[1;32m    162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mupdate_space(hp)\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Update the recorded scopes.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[80], line 10\u001b[0m, in \u001b[0;36mhyperparameter_lstm_model\u001b[0;34m(hp)\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m      7\u001b[0m \u001b[39m#model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True), input_shape=(126, 128)))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#model.add(CuDNNLSTM(4, input_shape=(126, 128), return_sequences=True))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m#model.add(Dropout(0.5))\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model\u001b[39m.\u001b[39;49madd(LSTM(\u001b[39m32\u001b[39;49m, input_shape\u001b[39m=\u001b[39;49m(\u001b[39m126\u001b[39;49m, \u001b[39m13\u001b[39;49m), return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39madd(Dropout(hp\u001b[39m.\u001b[39mFloat(\u001b[39m'\u001b[39m\u001b[39mdropout_lstm\u001b[39m\u001b[39m'\u001b[39m, min_value\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, max_value\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, step\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)))\n\u001b[1;32m     12\u001b[0m model\u001b[39m.\u001b[39madd(LSTM(units \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mChoice(\u001b[39m'\u001b[39m\u001b[39mlstm_layer_units_1\u001b[39m\u001b[39m'\u001b[39m, values \u001b[39m=\u001b[39m [\u001b[39m32\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m128\u001b[39m]), return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/sequential.py:222\u001b[0m, in \u001b[0;36mSequential.add\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    214\u001b[0m         x \u001b[39m=\u001b[39m input_layer\u001b[39m.\u001b[39mInput(\n\u001b[1;32m    215\u001b[0m             batch_shape\u001b[39m=\u001b[39mbatch_shape,\n\u001b[1;32m    216\u001b[0m             dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m    217\u001b[0m             name\u001b[39m=\u001b[39mlayer\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_input\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    218\u001b[0m         )\n\u001b[1;32m    219\u001b[0m         \u001b[39m# This will build the current layer\u001b[39;00m\n\u001b[1;32m    220\u001b[0m         \u001b[39m# and create the node connecting the current layer\u001b[39;00m\n\u001b[1;32m    221\u001b[0m         \u001b[39m# to the input layer we just created.\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m         layer(x)\n\u001b[1;32m    223\u001b[0m         set_inputs \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m set_inputs:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/layers/rnn/base_rnn.py:556\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m inputs, initial_state, constants \u001b[39m=\u001b[39m rnn_utils\u001b[39m.\u001b[39mstandardize_args(\n\u001b[1;32m    552\u001b[0m     inputs, initial_state, constants, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_constants\n\u001b[1;32m    553\u001b[0m )\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 556\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m \u001b[39m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[39m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39m# input_spec to include them.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/base_layer.py:1045\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[39m# Functional Model construction mode is invoked when `Layer`s are called\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m# on symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m _in_functional_construction_mode(\n\u001b[1;32m   1043\u001b[0m     \u001b[39mself\u001b[39m, inputs, args, kwargs, input_list\n\u001b[1;32m   1044\u001b[0m ):\n\u001b[0;32m-> 1045\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_functional_construction_call(\n\u001b[1;32m   1046\u001b[0m         inputs, args, kwargs, input_list\n\u001b[1;32m   1047\u001b[0m     )\n\u001b[1;32m   1049\u001b[0m \u001b[39m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m call_context \u001b[39m=\u001b[39m base_layer_utils\u001b[39m.\u001b[39mcall_context()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/base_layer.py:2535\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   2528\u001b[0m         training_arg_passed_by_framework \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2530\u001b[0m \u001b[39mwith\u001b[39;00m call_context\u001b[39m.\u001b[39menter(\n\u001b[1;32m   2531\u001b[0m     layer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, inputs\u001b[39m=\u001b[39minputs, build_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39mtraining_value\n\u001b[1;32m   2532\u001b[0m ):\n\u001b[1;32m   2533\u001b[0m     \u001b[39m# Check input assumptions set after layer building, e.g. input\u001b[39;00m\n\u001b[1;32m   2534\u001b[0m     \u001b[39m# shape.\u001b[39;00m\n\u001b[0;32m-> 2535\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keras_tensor_symbolic_call(\n\u001b[1;32m   2536\u001b[0m         inputs, input_masks, args, kwargs\n\u001b[1;32m   2537\u001b[0m     )\n\u001b[1;32m   2539\u001b[0m     \u001b[39mif\u001b[39;00m outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2540\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2541\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA layer\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms `call` method should return a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2542\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTensor or a list of Tensors, not None \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2543\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(layer: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2544\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/base_layer.py:2382\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m   2378\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m   2379\u001b[0m         keras_tensor\u001b[39m.\u001b[39mKerasTensor, output_signature\n\u001b[1;32m   2380\u001b[0m     )\n\u001b[1;32m   2381\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2382\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_output_signature(\n\u001b[1;32m   2383\u001b[0m         inputs, args, kwargs, input_masks\n\u001b[1;32m   2384\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/base_layer.py:2439\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m   2431\u001b[0m \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_scope()):\n\u001b[1;32m   2432\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   2433\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   2434\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2437\u001b[0m         \u001b[39m# TODO(kaftan): do we maybe_build here, or have we already\u001b[39;00m\n\u001b[1;32m   2438\u001b[0m         \u001b[39m# done it?\u001b[39;00m\n\u001b[0;32m-> 2439\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_build(inputs)\n\u001b[1;32m   2440\u001b[0m         inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[1;32m   2441\u001b[0m         outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/base_layer.py:2986\u001b[0m, in \u001b[0;36mLayer._maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2981\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild, \u001b[39m\"\u001b[39m\u001b[39m_is_default\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2982\u001b[0m     \u001b[39m# Any setup work performed only once should happen in an\u001b[39;00m\n\u001b[1;32m   2983\u001b[0m     \u001b[39m# `init_scope` to avoid creating symbolic Tensors that will\u001b[39;00m\n\u001b[1;32m   2984\u001b[0m     \u001b[39m# later pollute any eager operations.\u001b[39;00m\n\u001b[1;32m   2985\u001b[0m     \u001b[39mwith\u001b[39;00m tf_utils\u001b[39m.\u001b[39mmaybe_init_scope(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 2986\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild(input_shapes)\n\u001b[1;32m   2987\u001b[0m \u001b[39m# We must set also ensure that the layer is marked as built, and the\u001b[39;00m\n\u001b[1;32m   2988\u001b[0m \u001b[39m# build shape is stored since user defined build functions may not\u001b[39;00m\n\u001b[1;32m   2989\u001b[0m \u001b[39m# be calling `super.build()`\u001b[39;00m\n\u001b[1;32m   2990\u001b[0m Layer\u001b[39m.\u001b[39mbuild(\u001b[39mself\u001b[39m, input_shapes)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/layers/rnn/base_rnn.py:461\u001b[0m, in \u001b[0;36mRNN.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell, base_layer\u001b[39m.\u001b[39mLayer) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m    460\u001b[0m     \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell\u001b[39m.\u001b[39mname):\n\u001b[0;32m--> 461\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcell\u001b[39m.\u001b[39;49mbuild(step_input_shape)\n\u001b[1;32m    462\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell\u001b[39m.\u001b[39mbuilt \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m# set or validate state_spec\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/tf_utils.py:354\u001b[0m, in \u001b[0;36mshape_type_conversion.<locals>.wrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mif\u001b[39;00m input_shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     input_shape \u001b[39m=\u001b[39m convert_shapes(input_shape, to_tuples\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 354\u001b[0m output_shape \u001b[39m=\u001b[39m fn(instance, input_shape)\n\u001b[1;32m    355\u001b[0m \u001b[39m# Return shapes from `fn` as TensorShapes.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m output_shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/layers/rnn/lstm.py:199\u001b[0m, in \u001b[0;36mLSTMCell.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    190\u001b[0m input_dim \u001b[39m=\u001b[39m input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_weight(\n\u001b[1;32m    192\u001b[0m     shape\u001b[39m=\u001b[39m(input_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m),\n\u001b[1;32m    193\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m     caching_device\u001b[39m=\u001b[39mdefault_caching_device,\n\u001b[1;32m    198\u001b[0m )\n\u001b[0;32m--> 199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecurrent_kernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_weight(\n\u001b[1;32m    200\u001b[0m     shape\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munits, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munits \u001b[39m*\u001b[39;49m \u001b[39m4\u001b[39;49m),\n\u001b[1;32m    201\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrecurrent_kernel\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    202\u001b[0m     initializer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecurrent_initializer,\n\u001b[1;32m    203\u001b[0m     regularizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecurrent_regularizer,\n\u001b[1;32m    204\u001b[0m     constraint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecurrent_constraint,\n\u001b[1;32m    205\u001b[0m     caching_device\u001b[39m=\u001b[39;49mdefault_caching_device,\n\u001b[1;32m    206\u001b[0m )\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_bias:\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_forget_bias:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/base_layer.py:711\u001b[0m, in \u001b[0;36mLayer.add_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39mif\u001b[39;00m layout:\n\u001b[1;32m    709\u001b[0m     getter \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(getter, layout\u001b[39m=\u001b[39mlayout)\n\u001b[0;32m--> 711\u001b[0m variable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_variable_with_custom_getter(\n\u001b[1;32m    712\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    713\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    714\u001b[0m     \u001b[39m# TODO(allenl): a `make_variable` equivalent should be added as a\u001b[39;49;00m\n\u001b[1;32m    715\u001b[0m     \u001b[39m# `Trackable` method.\u001b[39;49;00m\n\u001b[1;32m    716\u001b[0m     getter\u001b[39m=\u001b[39;49mgetter,\n\u001b[1;32m    717\u001b[0m     \u001b[39m# Manage errors in Layer rather than Trackable.\u001b[39;49;00m\n\u001b[1;32m    718\u001b[0m     overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    719\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m    720\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    721\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    722\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    723\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    724\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections_arg,\n\u001b[1;32m    725\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    726\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m    727\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    728\u001b[0m )\n\u001b[1;32m    729\u001b[0m \u001b[39mif\u001b[39;00m regularizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[39m# TODO(fchollet): in the future, this should be handled at the\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[39m# level of variable creation, and weight regularization losses\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[39m# should be variable attributes.\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     name_in_scope \u001b[39m=\u001b[39m variable\u001b[39m.\u001b[39mname[: variable\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:489\u001b[0m, in \u001b[0;36mTrackable._add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    479\u001b[0m   \u001b[39mif\u001b[39;00m (checkpoint_initializer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    480\u001b[0m       \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(initializer, CheckpointInitialValueCallable) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    481\u001b[0m            (initializer\u001b[39m.\u001b[39mrestore_uid \u001b[39m>\u001b[39m checkpoint_initializer\u001b[39m.\u001b[39mrestore_uid))):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39m# then we'll catch that when we call _track_trackable. So this is\u001b[39;00m\n\u001b[1;32m    487\u001b[0m     \u001b[39m# \"best effort\" to set the initializer with the highest restore UID.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     initializer \u001b[39m=\u001b[39m checkpoint_initializer\n\u001b[0;32m--> 489\u001b[0m new_variable \u001b[39m=\u001b[39m getter(\n\u001b[1;32m    490\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    491\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    492\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    493\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m    494\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_for_getter)\n\u001b[1;32m    496\u001b[0m \u001b[39m# If we set an initializer and the variable processed it, tracking will not\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39m# assign again. It will add this variable to our dependencies, and if there\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39m# is a non-trivial restoration queued, it will handle that. This also\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[39m# handles slot variables.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m overwrite \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(new_variable, Trackable):\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/base_layer_utils.py:134\u001b[0m, in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner, layout)\u001b[0m\n\u001b[1;32m    127\u001b[0m     use_resource \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m layout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[39m# In theory, in `use_resource` is True and `collections` is empty\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[39m# (that is to say, in TF2), we can use tf.Variable.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39m# However, this breaks legacy (Estimator) checkpoints because\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[39m# it changes variable names. Remove this when V1 is fully deprecated.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m tf1\u001b[39m.\u001b[39;49mVariable(\n\u001b[1;32m    135\u001b[0m         initial_value\u001b[39m=\u001b[39;49minit_val,\n\u001b[1;32m    136\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    137\u001b[0m         trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    138\u001b[0m         caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    139\u001b[0m         dtype\u001b[39m=\u001b[39;49mvariable_dtype,\n\u001b[1;32m    140\u001b[0m         validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m    141\u001b[0m         constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    142\u001b[0m         use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    143\u001b[0m         collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m    144\u001b[0m         synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    145\u001b[0m         aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m    146\u001b[0m         shape\u001b[39m=\u001b[39;49mvariable_shape \u001b[39mif\u001b[39;49;00m variable_shape \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[39mreturn\u001b[39;00m dtensor\u001b[39m.\u001b[39mDVariable(\n\u001b[1;32m    150\u001b[0m         initial_value\u001b[39m=\u001b[39minit_val,\n\u001b[1;32m    151\u001b[0m         name\u001b[39m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         shape\u001b[39m=\u001b[39mvariable_shape \u001b[39mif\u001b[39;00m variable_shape \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/variables.py:269\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39m@traceback_utils\u001b[39m\u001b[39m.\u001b[39mfilter_traceback\n\u001b[1;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    268\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m VariableV1:\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_v1_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    270\u001b[0m   \u001b[39melif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m Variable:\n\u001b[1;32m    271\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v2_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/variables.py:210\u001b[0m, in \u001b[0;36mVariableMetaclass._variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m aggregation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m   aggregation \u001b[39m=\u001b[39m VariableAggregation\u001b[39m.\u001b[39mNONE\n\u001b[0;32m--> 210\u001b[0m \u001b[39mreturn\u001b[39;00m previous_getter(\n\u001b[1;32m    211\u001b[0m     initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[1;32m    212\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    213\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m    214\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m    215\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    216\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    217\u001b[0m     variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[1;32m    218\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    219\u001b[0m     expected_shape\u001b[39m=\u001b[39;49mexpected_shape,\n\u001b[1;32m    220\u001b[0m     import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[1;32m    221\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    222\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    223\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    224\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m    225\u001b[0m     shape\u001b[39m=\u001b[39;49mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/variables.py:203\u001b[0m, in \u001b[0;36mVariableMetaclass._variable_v1_call.<locals>.<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_variable_v1_call\u001b[39m(\u001b[39mcls\u001b[39m,\n\u001b[1;32m    187\u001b[0m                       initial_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    188\u001b[0m                       trainable\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m                       aggregation\u001b[39m=\u001b[39mVariableAggregation\u001b[39m.\u001b[39mNONE,\n\u001b[1;32m    201\u001b[0m                       shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    202\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m   previous_getter \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: default_variable_creator(\u001b[39mNone\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    204\u001b[0m   \u001b[39mfor\u001b[39;00m _, getter \u001b[39min\u001b[39;00m ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_variable_creator_stack:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     previous_getter \u001b[39m=\u001b[39m _make_getter(getter, previous_getter)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/variable_scope.py:2706\u001b[0m, in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2704\u001b[0m \u001b[39mif\u001b[39;00m use_resource:\n\u001b[1;32m   2705\u001b[0m   distribute_strategy \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdistribute_strategy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 2706\u001b[0m   \u001b[39mreturn\u001b[39;00m resource_variable_ops\u001b[39m.\u001b[39;49mResourceVariable(\n\u001b[1;32m   2707\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[1;32m   2708\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m   2709\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m   2710\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m   2711\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m   2712\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   2713\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   2714\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m   2715\u001b[0m       variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[1;32m   2716\u001b[0m       import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[1;32m   2717\u001b[0m       distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[1;32m   2718\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m   2719\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m   2720\u001b[0m       shape\u001b[39m=\u001b[39;49mshape)\n\u001b[1;32m   2721\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2722\u001b[0m   \u001b[39mreturn\u001b[39;00m variables\u001b[39m.\u001b[39mRefVariable(\n\u001b[1;32m   2723\u001b[0m       initial_value\u001b[39m=\u001b[39minitial_value,\n\u001b[1;32m   2724\u001b[0m       trainable\u001b[39m=\u001b[39mtrainable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2735\u001b[0m       aggregation\u001b[39m=\u001b[39maggregation,\n\u001b[1;32m   2736\u001b[0m       shape\u001b[39m=\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/variables.py:273\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v2_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    272\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(VariableMetaclass, \u001b[39mcls\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:1721\u001b[0m, in \u001b[0;36mResourceVariable.__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape, handle, experimental_enable_variable_lifting)\u001b[0m\n\u001b[1;32m   1716\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_from_handle(trainable\u001b[39m=\u001b[39mtrainable,\n\u001b[1;32m   1717\u001b[0m                          shape\u001b[39m=\u001b[39mshape,\n\u001b[1;32m   1718\u001b[0m                          dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1719\u001b[0m                          handle\u001b[39m=\u001b[39mhandle)\n\u001b[1;32m   1720\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1721\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_from_args(\n\u001b[1;32m   1722\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[1;32m   1723\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m   1724\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m   1725\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m   1726\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1727\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1728\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m   1729\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m   1730\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m   1731\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m   1732\u001b[0m       distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[1;32m   1733\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m   1734\u001b[0m       experimental_enable_variable_lifting\u001b[39m=\u001b[39;49mexperimental_enable_variable_lifting,\n\u001b[1;32m   1735\u001b[0m       )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:1905\u001b[0m, in \u001b[0;36mResourceVariable._init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape, validate_shape, experimental_enable_variable_lifting)\u001b[0m\n\u001b[1;32m   1903\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mInitializer\u001b[39m\u001b[39m\"\u001b[39m), device_context_manager(\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1904\u001b[0m   \u001b[39mif\u001b[39;00m init_from_fn:\n\u001b[0;32m-> 1905\u001b[0m     initial_value \u001b[39m=\u001b[39m initial_value()\n\u001b[1;32m   1906\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(initial_value, trackable\u001b[39m.\u001b[39mCheckpointInitialValue):\n\u001b[1;32m   1907\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_initialize_trackable()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:750\u001b[0m, in \u001b[0;36mOrthogonal.__call__\u001b[0;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m     _ensure_keras_seeded()\n\u001b[1;32m    747\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mcall_with_layout(\n\u001b[1;32m    748\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_init_val, layout, shape\u001b[39m=\u001b[39mshape, dtype\u001b[39m=\u001b[39mdtype\n\u001b[1;32m    749\u001b[0m     )\n\u001b[0;32m--> 750\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_init_val(shape, dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:767\u001b[0m, in \u001b[0;36mOrthogonal._generate_init_val\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39m# Make Q uniform\u001b[39;00m\n\u001b[1;32m    766\u001b[0m d \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mtensor_diag_part(r)\n\u001b[0;32m--> 767\u001b[0m q \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49msign(d)\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m num_rows \u001b[39m<\u001b[39m num_cols:\n\u001b[1;32m    769\u001b[0m     q \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mmatrix_transpose(q)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:790\u001b[0m, in \u001b[0;36msign\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_complex:\n\u001b[1;32m    781\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39mdiv_no_nan(\n\u001b[1;32m    782\u001b[0m       x,\n\u001b[1;32m    783\u001b[0m       cast(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    788\u001b[0m           dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype),\n\u001b[1;32m    789\u001b[0m       name\u001b[39m=\u001b[39mname)\n\u001b[0;32m--> 790\u001b[0m \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49msign(x, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:9579\u001b[0m, in \u001b[0;36msign\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   9577\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   9578\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 9579\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   9580\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mSign\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x)\n\u001b[1;32m   9581\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   9582\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if (global_model_type == 'cnn'):\n",
    "    print('Start running cnn')\n",
    "    run_cnn_5_fold_cross_validation()\n",
    "elif (global_model_type == 'lstm'):\n",
    "    print('Start running lstm')\n",
    "    run_lstm_5_fold_cross_validation()\n",
    "elif (global_model_type == 'tune_cnn'):\n",
    "    print('Start tuning cnn')\n",
    "    tune_cnn()\n",
    "elif (global_model_type == 'tune_lstm'):\n",
    "    print('Start tuning lstm')\n",
    "    tune_lstm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
